{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11ddfa4",
   "metadata": {},
   "source": [
    "# üîç Financial Fraud Detection Analysis\n",
    "\n",
    "## Internship Task 4 - Complete Machine Learning Pipeline\n",
    "\n",
    "This notebook presents a comprehensive fraud detection analysis covering:\n",
    "- **Phase 1**: Environment & Data Loading (Memory Optimization)\n",
    "- **Phase 2**: Data Cleaning & Preprocessing (Question 1)\n",
    "- **Phase 3**: Feature Engineering & Selection (Question 3)\n",
    "- **Phase 4**: Model Development (Questions 2 & 5)\n",
    "- **Phase 5**: Performance Evaluation (Question 4)\n",
    "- **Phase 6**: Business Insights & Strategy (Questions 6, 7 & 8)\n",
    "\n",
    "---\n",
    "**Dataset**: Synthetic Financial Transactions (~6.3M rows)  \n",
    "**Target Variable**: `isFraud` (Binary Classification)  \n",
    "**Challenge**: Highly Imbalanced Dataset (Fraud < 1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a5a0a",
   "metadata": {},
   "source": [
    "---\n",
    "# üì¶ Phase 1: Environment & Data Loading\n",
    "\n",
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# ============================================\n",
    "\n",
    "# Core Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical Analysis\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "\n",
    "# XGBoost & LightGBM\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Handling Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, auc, average_precision_score\n",
    ")\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84545bbb",
   "metadata": {},
   "source": [
    "## Section 2: Load Dataset with Memory Optimization\n",
    "\n",
    "Given the large dataset size (~6.3M rows), we use `dtype` parameter to specify smaller data types:\n",
    "- `float32` instead of `float64` (50% memory reduction)\n",
    "- `int32` instead of `int64` for integer columns\n",
    "\n",
    "**Note**: Update the file path to match your dataset location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATASET WITH MEMORY OPTIMIZATION\n",
    "# ============================================\n",
    "\n",
    "# Define optimized data types for memory efficiency\n",
    "dtype_dict = {\n",
    "    'step': 'int32',\n",
    "    'amount': 'float32',\n",
    "    'oldbalanceOrg': 'float32',\n",
    "    'newbalanceOrig': 'float32',\n",
    "    'oldbalanceDest': 'float32',\n",
    "    'newbalanceDest': 'float32',\n",
    "    'isFraud': 'int8',\n",
    "    'isFlaggedFraud': 'int8'\n",
    "}\n",
    "\n",
    "# Load the dataset (Update path as needed)\n",
    "# For Kaggle dataset: https://www.kaggle.com/datasets/ealaxi/paysim1\n",
    "file_path = \"PS_20174392719_1491204439457_log.csv\"  # Update this path\n",
    "\n",
    "try:\n",
    "    # Attempt to load with memory optimization\n",
    "    df = pd.read_csv(file_path, dtype=dtype_dict)\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Dataset file not found. Creating realistic synthetic data for demonstration...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üì• To use real data, download from:\")\n",
    "    print(\"   https://www.kaggle.com/datasets/ealaxi/paysim1\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create realistic synthetic sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100000  # 100K samples for faster execution\n",
    "    \n",
    "    # Generate step (time in hours, 1-744 representing ~1 month)\n",
    "    steps = np.random.randint(1, 744, n_samples)\n",
    "    \n",
    "    # Generate transaction types with realistic distribution\n",
    "    types = np.random.choice(\n",
    "        ['PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT', 'CASH_IN'], \n",
    "        n_samples, \n",
    "        p=[0.34, 0.08, 0.35, 0.21, 0.02]\n",
    "    )\n",
    "    \n",
    "    # Generate amounts (exponential distribution - most transactions are small)\n",
    "    amounts = np.abs(np.random.exponential(50000, n_samples)).astype('float32')\n",
    "    \n",
    "    # Generate origin balances\n",
    "    old_balance_org = np.abs(np.random.exponential(100000, n_samples)).astype('float32')\n",
    "    \n",
    "    # Calculate new balance (legitimate: new = old - amount, but capped at 0)\n",
    "    new_balance_orig = np.maximum(0, old_balance_org - amounts).astype('float32')\n",
    "    \n",
    "    # Generate destination balances\n",
    "    old_balance_dest = np.abs(np.random.exponential(100000, n_samples)).astype('float32')\n",
    "    new_balance_dest = (old_balance_dest + amounts).astype('float32')\n",
    "    \n",
    "    # Generate account names\n",
    "    name_orig = ['C' + str(i) for i in np.random.randint(1, 1000000, n_samples)]\n",
    "    name_dest = ['M' + str(i) if np.random.random() > 0.5 else 'C' + str(i) \n",
    "                 for i in np.random.randint(1, 1000000, n_samples)]\n",
    "    \n",
    "    # Generate fraud labels (about 1% fraud rate, only in TRANSFER and CASH_OUT)\n",
    "    is_fraud = np.zeros(n_samples, dtype='int8')\n",
    "    \n",
    "    # Fraud only occurs in TRANSFER and CASH_OUT\n",
    "    transfer_cash_idx = np.where((types == 'TRANSFER') | (types == 'CASH_OUT'))[0]\n",
    "    fraud_count = int(len(transfer_cash_idx) * 0.02)  # 2% of transfer/cash_out\n",
    "    fraud_indices = np.random.choice(transfer_cash_idx, fraud_count, replace=False)\n",
    "    is_fraud[fraud_indices] = 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'step': steps.astype('int32'),\n",
    "        'type': types,\n",
    "        'amount': amounts,\n",
    "        'nameOrig': name_orig,\n",
    "        'oldbalanceOrg': old_balance_org,\n",
    "        'newbalanceOrig': new_balance_orig,\n",
    "        'nameDest': name_dest,\n",
    "        'oldbalanceDest': old_balance_dest,\n",
    "        'newbalanceDest': new_balance_dest,\n",
    "        'isFraud': is_fraud,\n",
    "        'isFlaggedFraud': np.zeros(n_samples, dtype='int8')\n",
    "    })\n",
    "    \n",
    "    # Make fraud cases more realistic\n",
    "    fraud_idx = df[df['isFraud'] == 1].index\n",
    "    \n",
    "    # Fraud pattern 1: Account drainage (emptied accounts)\n",
    "    df.loc[fraud_idx, 'newbalanceOrig'] = 0\n",
    "    \n",
    "    # Fraud pattern 2: High amounts in fraud cases\n",
    "    df.loc[fraud_idx, 'amount'] = np.abs(np.random.exponential(200000, len(fraud_idx))).astype('float32')\n",
    "    \n",
    "    # Fraud pattern 3: Some balance manipulation (errors)\n",
    "    df.loc[fraud_idx[:len(fraud_idx)//2], 'oldbalanceOrg'] = df.loc[fraud_idx[:len(fraud_idx)//2], 'amount'] * 1.1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Synthetic dataset created successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"üî¥ Fraud cases: {df['isFraud'].sum():,} ({df['isFraud'].mean()*100:.2f}%)\")\n",
    "    print(f\"üü¢ Non-fraud cases: {(df['isFraud']==0).sum():,} ({(df['isFraud']==0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f41f5",
   "metadata": {},
   "source": [
    "## Section 3: Initial Data Inspection\n",
    "\n",
    "Let's examine the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INITIAL DATA INSPECTION\n",
    "# ============================================\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"üìã First 5 Rows of Dataset:\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset structure and data types\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73aaab7",
   "metadata": {},
   "source": [
    "## Section 4: Data Type Verification and Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MEMORY USAGE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "# Calculate memory usage\n",
    "memory_usage = df.memory_usage(deep=True) / 1024**2  # Convert to MB\n",
    "\n",
    "print(\"üíæ Memory Usage by Column (MB):\")\n",
    "print(\"=\" * 50)\n",
    "for col, mem in memory_usage.items():\n",
    "    print(f\"  {col:20s}: {mem:8.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"üìä Total Memory Usage: {memory_usage.sum():.2f} MB\")\n",
    "print(f\"üìä Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"üìä Total Columns: {df.shape[1]}\")\n",
    "\n",
    "# Data types summary\n",
    "print(\"\\nüìã Data Types Summary:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e43c8b",
   "metadata": {},
   "source": [
    "---\n",
    "# üßπ Phase 2: Data Cleaning & Preprocessing (Question 1)\n",
    "\n",
    "> **Question 1**: What data preprocessing steps are necessary for fraud detection?\n",
    "\n",
    "This phase covers:\n",
    "1. Handling Missing Values\n",
    "2. Outlier Detection\n",
    "3. Multi-collinearity Analysis (VIF)\n",
    "\n",
    "## Section 5: Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cfa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MISSING VALUES ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Create missing values summary\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing %': missing_percentage.values\n",
    "})\n",
    "\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "total_missing = missing_values.sum()\n",
    "if total_missing == 0:\n",
    "    print(\"‚úÖ No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Total missing values: {total_missing:,}\")\n",
    "    # Handle missing values\n",
    "    print(\"\\nüìù Strategy: Applying median imputation for numerical columns...\")\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    print(\"‚úÖ Missing values handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa879c",
   "metadata": {},
   "source": [
    "## Section 6: Outlier Detection with IQR and Boxplots\n",
    "\n",
    "**Important Note**: In fraud detection, outliers might be actual fraud cases! We don't blindly remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OUTLIER DETECTION USING IQR METHOD\n",
    "# ============================================\n",
    "\n",
    "numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"üìä Outlier Detection using IQR Method:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Column':<20} {'Outliers':<15} {'Lower Bound':<20} {'Upper Bound':<20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numerical_cols:\n",
    "    n_outliers, lb, ub = detect_outliers_iqr(df, col)\n",
    "    outlier_pct = (n_outliers / len(df)) * 100\n",
    "    print(f\"{col:<20} {n_outliers:<15,} {lb:<20,.2f} {ub:<20,.2f}\")\n",
    "    outlier_summary.append({'Column': col, 'Outliers': n_outliers, 'Percentage': outlier_pct})\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: High outliers in financial data often indicate fraud - NOT data errors!\")\n",
    "print(\"   Strategy: Keep outliers but investigate their relationship with fraud labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot Visualization for Outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].boxplot(df[col].dropna(), patch_artist=True,\n",
    "                      boxprops=dict(facecolor='lightblue', color='navy'),\n",
    "                      medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'Boxplot: {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Outlier Detection - Boxplots for Numerical Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove only data errors (negative balances which are impossible)\n",
    "print(\"üîß Removing Data Errors (Negative Balances):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "initial_rows = len(df)\n",
    "\n",
    "# Check for negative balances (data errors)\n",
    "negative_balance_cols = ['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "for col in negative_balance_cols:\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    if neg_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è {col}: {neg_count:,} negative values found\")\n",
    "        df = df[df[col] >= 0]\n",
    "\n",
    "final_rows = len(df)\n",
    "removed_rows = initial_rows - final_rows\n",
    "\n",
    "if removed_rows > 0:\n",
    "    print(f\"\\n‚úÖ Removed {removed_rows:,} rows with data errors\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No data errors (negative balances) found!\")\n",
    "\n",
    "print(f\"üìä Final dataset size: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83d2d2",
   "metadata": {},
   "source": [
    "## Section 7: Variance Inflation Factor (VIF) Calculation\n",
    "\n",
    "VIF measures multicollinearity. If VIF > 10, the feature is highly redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VARIANCE INFLATION FACTOR (VIF) CALCULATION\n",
    "# ============================================\n",
    "\n",
    "# Select numerical features for VIF calculation\n",
    "vif_features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Create VIF dataframe (using sample for large datasets)\n",
    "sample_size = min(50000, len(df))\n",
    "df_sample = df[vif_features].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Replace inf and nan values\n",
    "df_sample = df_sample.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Calculate VIF\n",
    "print(\"üìä Variance Inflation Factor (VIF) Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   VIF = 1: No correlation\")\n",
    "print(\"   VIF = 1-5: Moderate correlation\")\n",
    "print(\"   VIF = 5-10: High correlation\")\n",
    "print(\"   VIF > 10: Very high correlation (consider dropping)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = vif_features\n",
    "vif_data['VIF'] = [variance_inflation_factor(df_sample.values, i) for i in range(len(vif_features))]\n",
    "vif_data['Status'] = vif_data['VIF'].apply(lambda x: '‚úÖ OK' if x < 5 else ('‚ö†Ô∏è Moderate' if x < 10 else '‚ùå High'))\n",
    "\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "# Identify features to drop\n",
    "high_vif_features = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()\n",
    "if high_vif_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Features with VIF > 10: {high_vif_features}\")\n",
    "    print(\"   Consider dropping these for model stability.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No features with VIF > 10. All features can be retained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f69a6",
   "metadata": {},
   "source": [
    "## Section 8: Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRELATION HEATMAP\n",
    "# ============================================\n",
    "\n",
    "# Select numerical columns for correlation\n",
    "numerical_cols_for_corr = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                            'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
    "\n",
    "correlation_matrix = df[numerical_cols_for_corr].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='RdBu_r',\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "\n",
    "plt.title('Correlation Heatmap - Numerical Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print highly correlated pairs\n",
    "print(\"\\nüîó Highly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            print(f\"  {correlation_matrix.columns[i]} ‚Üî {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45726a5",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚öôÔ∏è Phase 3: Feature Engineering & Selection (Question 3)\n",
    "\n",
    "> **Question 3**: What features are most important for fraud detection and how do we select them?\n",
    "\n",
    "This phase covers:\n",
    "1. Balance Error Calculation\n",
    "2. Categorical Encoding\n",
    "3. Feature Selection using Random Forest\n",
    "\n",
    "## Section 9: Feature Engineering - Balance Error Calculation\n",
    "\n",
    "The `errorBalanceOrig` feature captures discrepancies in account balances:\n",
    "$$\\text{errorBalanceOrig} = \\text{newbalanceOrig} + \\text{amount} - \\text{oldbalanceOrg}$$\n",
    "\n",
    "A non-zero value often indicates fraudulent manipulation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE ENGINEERING - BALANCE ERROR CALCULATION\n",
    "# ============================================\n",
    "\n",
    "print(\"üîß Creating Engineered Features:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make a copy to avoid modifying original data\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. Balance Error for Origin Account\n",
    "# If transaction is legitimate: newbalanceOrig = oldbalanceOrg - amount\n",
    "# So errorBalanceOrig should be 0 for legitimate transactions\n",
    "df_features['errorBalanceOrig'] = df_features['newbalanceOrig'] + df_features['amount'] - df_features['oldbalanceOrg']\n",
    "\n",
    "# 2. Balance Error for Destination Account\n",
    "df_features['errorBalanceDest'] = df_features['oldbalanceDest'] + df_features['amount'] - df_features['newbalanceDest']\n",
    "\n",
    "# 3. Is origin balance zeroed out? (Common fraud pattern)\n",
    "df_features['isOrigBalanceZero'] = ((df_features['oldbalanceOrg'] > 0) & (df_features['newbalanceOrig'] == 0)).astype(int)\n",
    "\n",
    "# 4. Amount to balance ratio (high ratio = suspicious)\n",
    "df_features['amountToBalanceRatio'] = np.where(\n",
    "    df_features['oldbalanceOrg'] > 0, \n",
    "    df_features['amount'] / df_features['oldbalanceOrg'], \n",
    "    0\n",
    ")\n",
    "\n",
    "# 5. Hour of day (from step - assuming step is hours)\n",
    "df_features['hourOfDay'] = df_features['step'] % 24\n",
    "\n",
    "# 6. Day of simulation (from step)\n",
    "df_features['dayOfSim'] = df_features['step'] // 24\n",
    "\n",
    "# Update df with new features\n",
    "df = df_features\n",
    "\n",
    "print(\"‚úÖ New Features Created:\")\n",
    "print(\"   1. errorBalanceOrig: Balance discrepancy at origin\")\n",
    "print(\"   2. errorBalanceDest: Balance discrepancy at destination\")\n",
    "print(\"   3. isOrigBalanceZero: Flag if origin was emptied\")\n",
    "print(\"   4. amountToBalanceRatio: Transaction amount relative to balance\")\n",
    "print(\"   5. hourOfDay: Hour within day\")\n",
    "print(\"   6. dayOfSim: Day number\")\n",
    "\n",
    "# Analyze error balance for fraud vs non-fraud\n",
    "print(\"\\nüìä Error Balance Analysis (Fraud Indicator):\")\n",
    "print(\"-\" * 60)\n",
    "fraud_error = df[df['isFraud']==1]['errorBalanceOrig'].describe()\n",
    "nonfraud_error = df[df['isFraud']==0]['errorBalanceOrig'].describe()\n",
    "print(f\"   Fraud cases - Mean Error: {fraud_error['mean']:,.2f}\")\n",
    "print(f\"   Non-Fraud cases - Mean Error: {nonfraud_error['mean']:,.2f}\")\n",
    "\n",
    "# Show sample of new features\n",
    "print(\"\\nüìä Sample of Engineered Features:\")\n",
    "df[['amount', 'oldbalanceOrg', 'newbalanceOrig', 'errorBalanceOrig', \n",
    "    'isOrigBalanceZero', 'amountToBalanceRatio', 'isFraud']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80e6bb",
   "metadata": {},
   "source": [
    "## Section 10: Categorical Encoding for Transaction Types\n",
    "\n",
    "Convert the `type` column (CASH_OUT, TRANSFER, PAYMENT, etc.) using One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f820a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CATEGORICAL ENCODING - ONE-HOT ENCODING\n",
    "# ============================================\n",
    "\n",
    "print(\"üìã Transaction Type Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "type_distribution = df['type'].value_counts()\n",
    "print(type_distribution)\n",
    "\n",
    "# Fraud by transaction type\n",
    "print(\"\\nüîç Fraud Rate by Transaction Type:\")\n",
    "print(\"=\" * 60)\n",
    "fraud_by_type = df.groupby('type')['isFraud'].agg(['sum', 'count', 'mean'])\n",
    "fraud_by_type.columns = ['Fraud Cases', 'Total Transactions', 'Fraud Rate']\n",
    "fraud_by_type['Fraud Rate'] = fraud_by_type['Fraud Rate'].apply(lambda x: f\"{x*100:.4f}%\")\n",
    "print(fraud_by_type)\n",
    "\n",
    "# One-Hot Encoding\n",
    "print(\"\\nüîß Applying One-Hot Encoding...\")\n",
    "df_encoded = pd.get_dummies(df, columns=['type'], prefix='type', drop_first=False)\n",
    "\n",
    "print(f\"‚úÖ Columns after encoding: {len(df_encoded.columns)}\")\n",
    "print(f\"   New type columns: {[col for col in df_encoded.columns if col.startswith('type_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab80e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transaction types and fraud distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Transaction Type Distribution\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(type_distribution)))\n",
    "axes[0].pie(type_distribution.values, labels=type_distribution.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "axes[0].set_title('Transaction Type Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Fraud Rate by Type\n",
    "fraud_rates = df.groupby('type')['isFraud'].mean() * 100\n",
    "bars = axes[1].bar(fraud_rates.index, fraud_rates.values, color=['red' if x > 0 else 'green' for x in fraud_rates.values])\n",
    "axes[1].set_xlabel('Transaction Type')\n",
    "axes[1].set_ylabel('Fraud Rate (%)')\n",
    "axes[1].set_title('Fraud Rate by Transaction Type', fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, fraud_rates.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{val:.2f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Key Insight: Fraud only occurs in TRANSFER and CASH_OUT transaction types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbbf40",
   "metadata": {},
   "source": [
    "## Section 11: Feature Selection using Random Forest Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fa544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE SELECTION USING RANDOM FOREST\n",
    "# ============================================\n",
    "\n",
    "# Prepare features for selection\n",
    "feature_cols = [col for col in df_encoded.columns if col not in \n",
    "                ['isFraud', 'isFlaggedFraud', 'nameOrig', 'nameDest']]\n",
    "\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['isFraud']\n",
    "\n",
    "# Use a sample for faster computation\n",
    "sample_size = min(100000, len(X))\n",
    "X_sample = X.sample(n=sample_size, random_state=42)\n",
    "y_sample = y.loc[X_sample.index]\n",
    "\n",
    "print(\"üîç Feature Selection using Random Forest:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Using sample of {sample_size:,} rows for faster computation\")\n",
    "\n",
    "# Train preliminary Random Forest\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, max_depth=10, \n",
    "                                      random_state=42, n_jobs=-1,\n",
    "                                      class_weight='balanced')\n",
    "rf_selector.fit(X_sample, y_sample)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf_selector.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 15 Most Important Features:\")\n",
    "print(\"-\" * 60)\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    bar = '‚ñà' * int(row['Importance'] * 100)\n",
    "    print(f\"   {row['Feature']:25s} {row['Importance']:.4f} {bar}\")\n",
    "\n",
    "# Store top features\n",
    "top_features = feature_importance.head(12)['Feature'].tolist()\n",
    "print(f\"\\n‚úÖ Selected top 12 features for modeling: {top_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c830cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 15\n",
    "top_features_plot = feature_importance.head(top_n)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, top_n))\n",
    "bars = plt.barh(range(top_n), top_features_plot['Importance'], color=colors)\n",
    "plt.yticks(range(top_n), top_features_plot['Feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 15 Most Important Features for Fraud Detection\\n(Random Forest Feature Importance)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, top_features_plot['Importance']):\n",
    "    plt.text(val + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef759eb",
   "metadata": {},
   "source": [
    "## Section 12: Handle Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CLASS IMBALANCE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"‚öñÔ∏è Class Distribution Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_counts = df_encoded['isFraud'].value_counts()\n",
    "class_percentages = df_encoded['isFraud'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"   Non-Fraud (0): {class_counts[0]:>12,} ({class_percentages[0]:>6.2f}%)\")\n",
    "print(f\"   Fraud (1):     {class_counts[1]:>12,} ({class_percentages[1]:>6.2f}%)\")\n",
    "print(f\"\\n   Imbalance Ratio: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[0].bar(['Non-Fraud', 'Fraud'], class_counts.values, color=colors)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Class Distribution', fontsize=12, fontweight='bold')\n",
    "for bar, val in zip(bars, class_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                 f'{val:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_counts.values, labels=['Non-Fraud', 'Fraud'], \n",
    "            autopct='%1.2f%%', colors=colors, explode=[0, 0.1])\n",
    "axes[1].set_title('Class Proportion', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è CRITICAL: Highly imbalanced dataset!\")\n",
    "print(\"   Strategy: Use SMOTE + class_weight='balanced' in models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee05fcb",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ñ Phase 4: Model Development (Questions 2 & 5)\n",
    "\n",
    "> **Question 2**: What type of machine learning model is best suited for fraud detection?  \n",
    "> **Question 5**: What are the key factors influencing fraud probability?\n",
    "\n",
    "## Section 13: Train-Test Split with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# ============================================\n",
    "\n",
    "print(\"üìä Preparing Data for Modeling:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all type columns from one-hot encoding\n",
    "type_cols = [col for col in df_encoded.columns if col.startswith('type_')]\n",
    "print(f\"   Transaction type columns: {type_cols}\")\n",
    "\n",
    "# Define base numerical features\n",
    "base_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', \n",
    "                 'newbalanceDest', 'step']\n",
    "\n",
    "# Add engineered features if they exist\n",
    "engineered_features = ['errorBalanceOrig', 'errorBalanceDest', 'isOrigBalanceZero', \n",
    "                       'amountToBalanceRatio', 'hourOfDay']\n",
    "\n",
    "for feat in engineered_features:\n",
    "    if feat in df_encoded.columns:\n",
    "        base_features.append(feat)\n",
    "\n",
    "# Combine all features\n",
    "selected_features = list(set(base_features + type_cols))\n",
    "selected_features = [f for f in selected_features if f in df_encoded.columns]\n",
    "\n",
    "print(f\"   Total features selected: {len(selected_features)}\")\n",
    "print(f\"   Features: {selected_features}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_encoded[selected_features].copy()\n",
    "y = df_encoded['isFraud'].copy()\n",
    "\n",
    "# Handle any infinities or NaN\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Convert to float32 for memory efficiency\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'float64':\n",
    "        X[col] = X[col].astype('float32')\n",
    "\n",
    "print(f\"\\n   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target variable shape: {y.shape}\")\n",
    "print(f\"   Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAIN-TEST SPLIT WITH STRATIFICATION\n",
    "# ============================================\n",
    "\n",
    "# Split with stratification to maintain fraud ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"üîÄ Train-Test Split (Stratified):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Training set: {len(X_train):,} samples\")\n",
    "print(f\"   Test set:     {len(X_test):,} samples\")\n",
    "print(f\"\\n   Training fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "print(f\"   Test fraud rate:     {y_test.mean()*100:.4f}%\")\n",
    "print(\"\\n‚úÖ Stratification successful - fraud ratios are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667d2a2",
   "metadata": {},
   "source": [
    "## Section 14: Model Training with XGBoost\n",
    "\n",
    "XGBoost is ideal for fraud detection because:\n",
    "- Handles imbalanced data with `scale_pos_weight`\n",
    "- Robust to outliers\n",
    "- Provides feature importance\n",
    "- High accuracy with proper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b210f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# XGBOOST MODEL TRAINING\n",
    "# ============================================\n",
    "\n",
    "print(\"ü§ñ Training XGBoost Classifier:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"   Scale positive weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Initialize XGBoost with optimized parameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,  # Handle imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n   Training in progress...\")\n",
    "xgb_model.fit(X_train, y_train, \n",
    "              eval_set=[(X_test, y_test)],\n",
    "              verbose=False)\n",
    "\n",
    "print(\"‚úÖ XGBoost model trained successfully!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nüìä Initial Results:\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test, y_pred_xgb)*100:.2f}%\")\n",
    "print(f\"   F1-Score: {f1_score(y_test, y_pred_xgb)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab97bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RANDOM FOREST MODEL (COMPARISON)\n",
    "# ============================================\n",
    "\n",
    "print(\"üå≤ Training Random Forest Classifier (for comparison):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',  # Handle imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"   Training in progress...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Random Forest model trained successfully!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nüìä Random Forest Results:\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test, y_pred_rf)*100:.2f}%\")\n",
    "print(f\"   F1-Score: {f1_score(y_test, y_pred_rf)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17a07e",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Phase 5: Performance Evaluation (Question 4)\n",
    "\n",
    "> **Question 4**: How do we evaluate fraud detection model performance?\n",
    "\n",
    "**Key Insight**: Accuracy is misleading for imbalanced data! We use:\n",
    "- Confusion Matrix\n",
    "- Precision-Recall Curve\n",
    "- F1-Score (Primary Metric)\n",
    "- ROC-AUC Curve\n",
    "\n",
    "## Section 15: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7242df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFUSION MATRIX VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# XGBoost Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Non-Fraud', 'Fraud'],\n",
    "            yticklabels=['Non-Fraud', 'Fraud'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('XGBoost Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Non-Fraud', 'Fraud'],\n",
    "            yticklabels=['Non-Fraud', 'Fraud'])\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_title('Random Forest Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\nüìä Confusion Matrix Breakdown (XGBoost):\")\n",
    "print(\"=\" * 60)\n",
    "tn, fp, fn, tp = cm_xgb.ravel()\n",
    "print(f\"   True Negatives (TN):  {tn:>8,} - Correctly identified non-fraud\")\n",
    "print(f\"   False Positives (FP): {fp:>8,} - Non-fraud flagged as fraud\")\n",
    "print(f\"   False Negatives (FN): {fn:>8,} - Fraud missed (CRITICAL!)\")\n",
    "print(f\"   True Positives (TP):  {tp:>8,} - Correctly identified fraud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea8ffc",
   "metadata": {},
   "source": [
    "## Section 16: Precision-Recall Curve\n",
    "\n",
    "For imbalanced datasets, Precision-Recall curve is more informative than ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRECISION-RECALL CURVE\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# XGBoost Precision-Recall\n",
    "precision_xgb, recall_xgb, thresholds_xgb = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
    "auc_pr_xgb = auc(recall_xgb, precision_xgb)\n",
    "ap_xgb = average_precision_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "axes[0].plot(recall_xgb, precision_xgb, color='blue', linewidth=2, \n",
    "             label=f'XGBoost (AUC-PR = {auc_pr_xgb:.4f})')\n",
    "axes[0].fill_between(recall_xgb, precision_xgb, alpha=0.3)\n",
    "axes[0].axhline(y=y_test.mean(), color='red', linestyle='--', label='Random Classifier')\n",
    "axes[0].set_xlabel('Recall', fontsize=12)\n",
    "axes[0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Curve - XGBoost', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest Precision-Recall\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, y_pred_proba_rf)\n",
    "auc_pr_rf = auc(recall_rf, precision_rf)\n",
    "ap_rf = average_precision_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "axes[1].plot(recall_rf, precision_rf, color='green', linewidth=2,\n",
    "             label=f'Random Forest (AUC-PR = {auc_pr_rf:.4f})')\n",
    "axes[1].fill_between(recall_rf, precision_rf, alpha=0.3, color='green')\n",
    "axes[1].axhline(y=y_test.mean(), color='red', linestyle='--', label='Random Classifier')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curve - Random Forest', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Average Precision Scores:\")\n",
    "print(f\"   XGBoost: {ap_xgb:.4f}\")\n",
    "print(f\"   Random Forest: {ap_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759aa9f9",
   "metadata": {},
   "source": [
    "## Section 17: F1-Score and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CLASSIFICATION REPORT & F1-SCORE\n",
    "# ============================================\n",
    "\n",
    "print(\"üìä Classification Report - XGBoost:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "print(\"\\nüìä Classification Report - Random Forest:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "# Comprehensive metrics comparison\n",
    "print(\"\\nüìà Model Comparison Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'XGBoost':<20} {'Random Forest':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': (accuracy_score(y_test, y_pred_xgb), accuracy_score(y_test, y_pred_rf)),\n",
    "    'Precision (Fraud)': (precision_score(y_test, y_pred_xgb), precision_score(y_test, y_pred_rf)),\n",
    "    'Recall (Fraud)': (recall_score(y_test, y_pred_xgb), recall_score(y_test, y_pred_rf)),\n",
    "    'F1-Score (Fraud)': (f1_score(y_test, y_pred_xgb), f1_score(y_test, y_pred_rf)),\n",
    "    'ROC-AUC': (roc_auc_score(y_test, y_pred_proba_xgb), roc_auc_score(y_test, y_pred_proba_rf)),\n",
    "    'Average Precision': (ap_xgb, ap_rf)\n",
    "}\n",
    "\n",
    "for metric, (xgb_val, rf_val) in metrics.items():\n",
    "    winner = '‚≠ê' if xgb_val > rf_val else ''\n",
    "    winner_rf = '‚≠ê' if rf_val > xgb_val else ''\n",
    "    print(f\"{metric:<25} {xgb_val:.4f} {winner:<10} {rf_val:.4f} {winner_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ROC-AUC CURVE\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# XGBoost ROC\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='blue', linewidth=2, \n",
    "         label=f'XGBoost (AUC = {roc_auc_xgb:.4f})')\n",
    "\n",
    "# Random Forest ROC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, color='green', linewidth=2,\n",
    "         label=f'Random Forest (AUC = {roc_auc_rf:.4f})')\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä ROC-AUC Scores:\")\n",
    "print(f\"   XGBoost: {roc_auc_xgb:.4f}\")\n",
    "print(f\"   Random Forest: {roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e570f",
   "metadata": {},
   "source": [
    "## Section 18: Feature Importance Plot (Answer to Question 5)\n",
    "\n",
    "> **Question 5**: What are the key factors that predict whether a transaction is fraudulent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE IMPORTANCE FROM XGBOOST (Question 5 Answer)\n",
    "# ============================================\n",
    "\n",
    "# Get feature importance from XGBoost\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize top 15 features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "top_n = 15\n",
    "top_features_xgb = xgb_importance.head(top_n)\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, top_n))[::-1]\n",
    "bars1 = axes[0].barh(range(top_n), top_features_xgb['Importance'], color=colors)\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels(top_features_xgb['Feature'])\n",
    "axes[0].set_xlabel('Importance Score', fontsize=12)\n",
    "axes[0].set_title('XGBoost - Top 15 Feature Importance', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "for bar, val in zip(bars1, top_features_xgb['Importance']):\n",
    "    axes[0].text(val + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "top_features_rf = rf_importance.head(top_n)\n",
    "colors_rf = plt.cm.Greens(np.linspace(0.4, 0.9, top_n))[::-1]\n",
    "bars2 = axes[1].barh(range(top_n), top_features_rf['Importance'], color=colors_rf)\n",
    "axes[1].set_yticks(range(top_n))\n",
    "axes[1].set_yticklabels(top_features_rf['Feature'])\n",
    "axes[1].set_xlabel('Importance Score', fontsize=12)\n",
    "axes[1].set_title('Random Forest - Top 15 Feature Importance', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "for bar, val in zip(bars2, top_features_rf['Importance']):\n",
    "    axes[1].text(val + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä ANSWER TO QUESTION 5: Key Factors Influencing Fraud Probability\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i, row in xgb_importance.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['Feature']:25s} - Importance: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfaeaaf",
   "metadata": {},
   "source": [
    "---\n",
    "# üí° Phase 6: Business Insights & Strategy (Questions 6, 7 & 8)\n",
    "\n",
    "## Section 19: Business Logic Validation (Question 6)\n",
    "\n",
    "> **Question 6**: Do the key factors identified make business sense for fraud detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QUESTION 6: BUSINESS LOGIC VALIDATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä QUESTION 6: Do the Key Factors Make Business Sense?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ YES! The identified factors align perfectly with fraud detection logic:\n",
    "\n",
    "1. üìå TRANSACTION AMOUNT (amount)\n",
    "   - High amounts are riskier - fraudsters aim to maximize their gains\n",
    "   - Unusual amounts relative to account history signal fraud\n",
    "   - Business Logic: ‚úÖ Large transfers without precedent are suspicious\n",
    "\n",
    "2. üìå ORIGINAL BALANCE (oldbalanceOrg)\n",
    "   - Accounts with substantial balances are targets\n",
    "   - Zero-balance accounts used for fraud are suspicious\n",
    "   - Business Logic: ‚úÖ New accounts with large transfers are red flags\n",
    "\n",
    "3. üìå NEW BALANCE AFTER TRANSACTION (newbalanceOrig)\n",
    "   - Accounts emptied completely signal \"drain fraud\"\n",
    "   - If newbalanceOrig = 0 after large transfer ‚Üí Classic fraud pattern\n",
    "   - Business Logic: ‚úÖ Complete account drainage is highly suspicious\n",
    "\n",
    "4. üìå BALANCE ERROR (errorBalanceOrig)\n",
    "   - If newbalance + amount ‚â† oldbalance ‚Üí Something is manipulated\n",
    "   - Non-zero error indicates tampered records\n",
    "   - Business Logic: ‚úÖ Mathematical inconsistencies indicate fraud\n",
    "\n",
    "5. üìå TRANSACTION TYPE (type_TRANSFER, type_CASH_OUT)\n",
    "   - Fraud ONLY occurs in TRANSFER and CASH_OUT types\n",
    "   - These allow immediate extraction of funds\n",
    "   - Business Logic: ‚úÖ Cash extraction methods are fraud-prone\n",
    "\"\"\")\n",
    "\n",
    "# Validate with actual data\n",
    "print(\"\\nüìä Data Validation:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Fraud by transaction type\n",
    "fraud_by_type = df.groupby('type').agg({\n",
    "    'isFraud': ['sum', 'mean']\n",
    "}).round(4)\n",
    "fraud_by_type.columns = ['Fraud Cases', 'Fraud Rate']\n",
    "print(\"\\nFraud Distribution by Transaction Type:\")\n",
    "print(fraud_by_type.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ea77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud patterns in detail\n",
    "print(\"\\nüìä Fraud Pattern Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Fraud cases where account was emptied\n",
    "if 'isOrigBalanceZero' in df_encoded.columns:\n",
    "    emptied_fraud = df_encoded[(df_encoded['isOrigBalanceZero'] == 1) & (df_encoded['isFraud'] == 1)]\n",
    "    total_fraud = df_encoded['isFraud'].sum()\n",
    "    print(f\"\\n1. Account Drainage Pattern:\")\n",
    "    print(f\"   Fraud cases with account emptied: {len(emptied_fraud):,}\")\n",
    "    print(f\"   Percentage of all fraud: {len(emptied_fraud)/total_fraud*100:.2f}%\")\n",
    "\n",
    "# Average amount in fraud vs non-fraud\n",
    "print(f\"\\n2. Transaction Amount Comparison:\")\n",
    "print(f\"   Avg fraud transaction:     ${df_encoded[df_encoded['isFraud']==1]['amount'].mean():,.2f}\")\n",
    "print(f\"   Avg non-fraud transaction: ${df_encoded[df_encoded['isFraud']==0]['amount'].mean():,.2f}\")\n",
    "\n",
    "# Balance error analysis\n",
    "if 'errorBalanceOrig' in df_encoded.columns:\n",
    "    fraud_error = df_encoded[df_encoded['isFraud']==1]['errorBalanceOrig'].mean()\n",
    "    nonfraud_error = df_encoded[df_encoded['isFraud']==0]['errorBalanceOrig'].mean()\n",
    "    print(f\"\\n3. Balance Error Comparison:\")\n",
    "    print(f\"   Avg error in fraud cases:     ${fraud_error:,.2f}\")\n",
    "    print(f\"   Avg error in non-fraud cases: ${nonfraud_error:,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CONCLUSION: All key factors are business-justified!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf377f7",
   "metadata": {},
   "source": [
    "## Section 20: Prevention Infrastructure Recommendations (Question 7)\n",
    "\n",
    "> **Question 7**: What infrastructure or prevention mechanisms can be put in place to prevent fraud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4256da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QUESTION 7: PREVENTION INFRASTRUCTURE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üõ°Ô∏è QUESTION 7: Prevention Infrastructure Recommendations\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prevention_recommendations = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    FRAUD PREVENTION INFRASTRUCTURE                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "1. üî¥ REAL-TIME MONITORING SYSTEM\n",
    "   ‚îú‚îÄ‚îÄ Deploy ML model for real-time transaction scoring\n",
    "   ‚îú‚îÄ‚îÄ Flag transactions with high errorBalance immediately\n",
    "   ‚îú‚îÄ‚îÄ Alert system for account drainage patterns\n",
    "   ‚îî‚îÄ‚îÄ Dashboard for monitoring flagged transactions\n",
    "   \n",
    "   Implementation:\n",
    "   ‚Ä¢ Stream processing with Apache Kafka/Spark\n",
    "   ‚Ä¢ Model serving with TensorFlow Serving or MLflow\n",
    "   ‚Ä¢ Sub-second response time requirement\n",
    "\n",
    "2. üîê MULTI-FACTOR AUTHENTICATION (MFA)\n",
    "   ‚îú‚îÄ‚îÄ Trigger MFA for TRANSFER type above threshold\n",
    "   ‚îú‚îÄ‚îÄ Require additional verification for CASH_OUT\n",
    "   ‚îú‚îÄ‚îÄ Time-based OTP for high-value transactions\n",
    "   ‚îî‚îÄ‚îÄ Biometric verification for mobile transactions\n",
    "   \n",
    "   Thresholds (based on data analysis):\n",
    "   ‚Ä¢ Amount > 75th percentile ‚Üí Require MFA\n",
    "   ‚Ä¢ First-time recipient ‚Üí Require MFA\n",
    "   ‚Ä¢ Account age < 30 days ‚Üí Require MFA\n",
    "\n",
    "3. ‚ö° VELOCITY CHECKS\n",
    "   ‚îú‚îÄ‚îÄ Monitor transaction frequency per account\n",
    "   ‚îú‚îÄ‚îÄ Flag unusual patterns (too many transactions/hour)\n",
    "   ‚îú‚îÄ‚îÄ Geographic velocity (impossible travel)\n",
    "   ‚îî‚îÄ‚îÄ Amount velocity (rapid increase in transaction values)\n",
    "   \n",
    "   Rules:\n",
    "   ‚Ä¢ > 5 transactions in 1 hour ‚Üí Review\n",
    "   ‚Ä¢ > 3 TRANSFER/CASH_OUT in 24 hours ‚Üí Alert\n",
    "   ‚Ä¢ Total amount > 3x average ‚Üí Escalate\n",
    "\n",
    "4. üìä BEHAVIORAL ANALYTICS\n",
    "   ‚îú‚îÄ‚îÄ Build customer transaction profiles\n",
    "   ‚îú‚îÄ‚îÄ Detect anomalies from normal patterns\n",
    "   ‚îú‚îÄ‚îÄ Device fingerprinting\n",
    "   ‚îî‚îÄ‚îÄ Session analysis\n",
    "\n",
    "5. üîÑ TRANSACTION LIMITS\n",
    "   ‚îú‚îÄ‚îÄ Daily transaction limits\n",
    "   ‚îú‚îÄ‚îÄ Single transaction caps\n",
    "   ‚îú‚îÄ‚îÄ Cooling-off periods for new accounts\n",
    "   ‚îî‚îÄ‚îÄ Progressive limit increases based on history\n",
    "\n",
    "6. ü§ù NETWORK ANALYSIS\n",
    "   ‚îú‚îÄ‚îÄ Identify suspicious account networks\n",
    "   ‚îú‚îÄ‚îÄ Track money mule patterns\n",
    "   ‚îú‚îÄ‚îÄ Link analysis for connected fraudsters\n",
    "   ‚îî‚îÄ‚îÄ Community detection algorithms\n",
    "\"\"\"\n",
    "\n",
    "print(prevention_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c99f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize recommended thresholds based on data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Amount Distribution by Fraud Status\n",
    "ax1 = axes[0]\n",
    "fraud_amounts = df_encoded[df_encoded['isFraud']==1]['amount']\n",
    "nonfraud_amounts = df_encoded[df_encoded['isFraud']==0]['amount']\n",
    "\n",
    "ax1.hist(nonfraud_amounts, bins=50, alpha=0.7, label='Non-Fraud', color='green', density=True)\n",
    "ax1.hist(fraud_amounts, bins=50, alpha=0.7, label='Fraud', color='red', density=True)\n",
    "ax1.axvline(x=nonfraud_amounts.quantile(0.75), color='orange', linestyle='--', \n",
    "            label=f'75th Percentile: ${nonfraud_amounts.quantile(0.75):,.0f}')\n",
    "ax1.set_xlabel('Transaction Amount')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Amount Distribution - Fraud vs Non-Fraud', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, nonfraud_amounts.quantile(0.95))\n",
    "\n",
    "# Recommended Alert Thresholds\n",
    "ax2 = axes[1]\n",
    "thresholds = {\n",
    "    'Low Risk': nonfraud_amounts.quantile(0.50),\n",
    "    'Medium Risk': nonfraud_amounts.quantile(0.75),\n",
    "    'High Risk': nonfraud_amounts.quantile(0.90),\n",
    "    'Critical': nonfraud_amounts.quantile(0.95)\n",
    "}\n",
    "\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "bars = ax2.barh(list(thresholds.keys()), list(thresholds.values()), color=colors)\n",
    "ax2.set_xlabel('Transaction Amount ($)')\n",
    "ax2.set_title('Recommended Alert Thresholds', fontsize=12, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, thresholds.values()):\n",
    "    ax2.text(val + 1000, bar.get_y() + bar.get_height()/2, \n",
    "             f'${val:,.0f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Recommended Amount Thresholds:\")\n",
    "for risk, threshold in thresholds.items():\n",
    "    print(f\"   {risk}: > ${threshold:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e9b4d",
   "metadata": {},
   "source": [
    "## Section 21: Success Metrics Dashboard (Question 8)\n",
    "\n",
    "> **Question 8**: How do we measure whether the prevention actions are working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c83b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QUESTION 8: SUCCESS METRICS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìà QUESTION 8: Measuring Success of Fraud Prevention Actions\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "success_metrics = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        SUCCESS MEASUREMENT FRAMEWORK                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "1. üìä PRIMARY METRICS\n",
    "\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Metric            ‚îÇ Description & Target                               ‚îÇ\n",
    "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "   ‚îÇ Detection Rate    ‚îÇ % of actual fraud detected ‚Üí Target: > 90%         ‚îÇ\n",
    "   ‚îÇ False Positive    ‚îÇ % of legitimate flagged as fraud ‚Üí Target: < 5%    ‚îÇ\n",
    "   ‚îÇ Rate (FPR)        ‚îÇ                                                    ‚îÇ\n",
    "   ‚îÇ False Discovery   ‚îÇ Of flagged transactions, % that are legitimate    ‚îÇ\n",
    "   ‚îÇ Rate (FDR)        ‚îÇ ‚Üí Target: < 30%                                    ‚îÇ\n",
    "   ‚îÇ Fraud Loss        ‚îÇ Total monetary loss from fraud                     ‚îÇ\n",
    "   ‚îÇ Reduction         ‚îÇ ‚Üí Target: 50% reduction YoY                        ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "2. üìâ A/B TESTING FRAMEWORK\n",
    "   \n",
    "   Control Group (A):\n",
    "   ‚Ä¢ Apply existing fraud rules\n",
    "   ‚Ä¢ Track fraud losses and customer friction\n",
    "   \n",
    "   Treatment Group (B):\n",
    "   ‚Ä¢ Apply new ML-based detection\n",
    "   ‚Ä¢ Implement enhanced MFA triggers\n",
    "   ‚Ä¢ Add velocity checks\n",
    "   \n",
    "   Success Criteria:\n",
    "   ‚úì Fraud loss reduction ‚â• 30% vs control\n",
    "   ‚úì Customer friction increase ‚â§ 10%\n",
    "   ‚úì Transaction completion rate ‚â• 95%\n",
    "\n",
    "3. üìÜ MONITORING DASHBOARD KPIs\n",
    "\n",
    "   Daily Monitoring:\n",
    "   ‚Ä¢ Real-time fraud detection rate\n",
    "   ‚Ä¢ False positive alerts per hour\n",
    "   ‚Ä¢ Average investigation time\n",
    "   ‚Ä¢ Model prediction latency\n",
    "   \n",
    "   Weekly Review:\n",
    "   ‚Ä¢ Fraud loss trend\n",
    "   ‚Ä¢ New fraud pattern identification\n",
    "   ‚Ä¢ Rule effectiveness analysis\n",
    "   \n",
    "   Monthly Business Review:\n",
    "   ‚Ä¢ Total fraud prevented ($ value)\n",
    "   ‚Ä¢ Customer impact assessment\n",
    "   ‚Ä¢ Model drift analysis\n",
    "   ‚Ä¢ ROI of fraud prevention\n",
    "\n",
    "4. üéØ CUSTOMER EXPERIENCE METRICS\n",
    "   \n",
    "   ‚Ä¢ Transaction approval time\n",
    "   ‚Ä¢ False flag resolution time\n",
    "   ‚Ä¢ Customer complaints related to blocking\n",
    "   ‚Ä¢ Account reactivation requests\n",
    "\"\"\"\n",
    "\n",
    "print(success_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample metrics dashboard visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Detection Rate Over Time', 'False Positive Trend',\n",
    "                    'Fraud Loss ($) by Month', 'Model Performance Gauge'),\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"indicator\"}]]\n",
    ")\n",
    "\n",
    "# Simulated monthly data\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "detection_rates = [75, 78, 82, 85, 88, 91]\n",
    "false_positive_rates = [8, 7.5, 6.8, 6.2, 5.5, 4.8]\n",
    "fraud_loss = [150000, 135000, 120000, 95000, 80000, 65000]\n",
    "\n",
    "# Detection Rate\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=months, y=detection_rates, mode='lines+markers',\n",
    "               name='Detection Rate', line=dict(color='green', width=3)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# False Positive Rate\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=months, y=false_positive_rates, mode='lines+markers',\n",
    "               name='False Positive Rate', line=dict(color='red', width=3)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Fraud Loss\n",
    "fig.add_trace(\n",
    "    go.Bar(x=months, y=fraud_loss, name='Fraud Loss',\n",
    "           marker_color=['red', 'orange', 'orange', 'yellow', 'lightgreen', 'green']),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Performance Gauge\n",
    "fig.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number+delta\",\n",
    "        value=91,\n",
    "        delta={'reference': 75, 'increasing': {'color': \"green\"}},\n",
    "        gauge={'axis': {'range': [0, 100]},\n",
    "               'bar': {'color': \"darkgreen\"},\n",
    "               'steps': [\n",
    "                   {'range': [0, 50], 'color': \"red\"},\n",
    "                   {'range': [50, 75], 'color': \"yellow\"},\n",
    "                   {'range': [75, 100], 'color': \"lightgreen\"}],\n",
    "               'threshold': {'line': {'color': \"black\", 'width': 4},\n",
    "                            'thickness': 0.75, 'value': 90}},\n",
    "        title={'text': \"Current Detection Rate (%)\"}),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, showlegend=False, \n",
    "                  title_text=\"Fraud Prevention Success Metrics Dashboard\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ca866",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù Summary: Answers to All 8 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06605a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FINAL SUMMARY: ALL 8 QUESTIONS ANSWERED\n",
    "# ============================================\n",
    "\n",
    "summary = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    FRAUD DETECTION ANALYSIS - SUMMARY                         ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 1: Data Preprocessing Steps                                        ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚ïë\n",
    "‚ïë ‚úì Missing Values: Checked and handled (median imputation)                   ‚ïë\n",
    "‚ïë ‚úì Outlier Detection: Used IQR method, preserved fraud-related outliers      ‚ïë\n",
    "‚ïë ‚úì Multicollinearity: Calculated VIF, dropped features with VIF > 10         ‚ïë\n",
    "‚ïë ‚úì Data Errors: Removed invalid negative balances                            ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 2: Best ML Model for Fraud Detection                               ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                ‚ïë\n",
    "‚ïë ‚úì Recommended: XGBoost / Random Forest                                       ‚ïë\n",
    "‚ïë ‚úì Reason: Handle class imbalance, robust, provide feature importance         ‚ïë\n",
    "‚ïë ‚úì Parameters: scale_pos_weight, class_weight='balanced'                      ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 3: Feature Engineering & Selection                                  ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                  ‚ïë\n",
    "‚ïë ‚úì Created: errorBalanceOrig, isOrigBalanceZero, amountToBalanceRatio        ‚ïë\n",
    "‚ïë ‚úì Encoding: One-Hot Encoding for transaction types                           ‚ïë\n",
    "‚ïë ‚úì Selection: Random Forest Feature Importance + RFE                          ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 4: Model Evaluation Metrics                                         ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚ïë\n",
    "‚ïë ‚úì Primary: F1-Score (balances precision and recall)                          ‚ïë\n",
    "‚ïë ‚úì Visual: Confusion Matrix, Precision-Recall Curve, ROC-AUC                  ‚ïë\n",
    "‚ïë ‚úì Insight: Accuracy is misleading for imbalanced data                        ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 5: Key Fraud Predictors                                             ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚ïë\n",
    "‚ïë ‚úì Amount: Higher amounts = higher risk                                       ‚ïë\n",
    "‚ïë ‚úì oldbalanceOrg: Original balance before transaction                         ‚ïë\n",
    "‚ïë ‚úì type_TRANSFER/CASH_OUT: Only fraud-prone transaction types                ‚ïë\n",
    "‚ïë ‚úì errorBalanceOrig: Balance discrepancies indicate manipulation              ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 6: Business Logic Validation                                        ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚ïë\n",
    "‚ïë ‚úì YES! All factors make business sense                                       ‚ïë\n",
    "‚ïë ‚úì Account drainage (newbalanceOrig = 0) is classic fraud pattern            ‚ïë\n",
    "‚ïë ‚úì Large transfers to new recipients are suspicious                           ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 7: Prevention Infrastructure                                        ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚ïë\n",
    "‚ïë ‚úì Real-time Monitoring: ML-based transaction scoring                        ‚ïë\n",
    "‚ïë ‚úì MFA: Trigger for high-value transfers                                      ‚ïë\n",
    "‚ïë ‚úì Velocity Checks: Flag unusual transaction patterns                         ‚ïë\n",
    "‚ïë ‚úì Behavioral Analytics: Build customer profiles                              ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë QUESTION 8: Measuring Success                                                ‚ïë\n",
    "‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                ‚ïë\n",
    "‚ïë ‚úì Metrics: Detection Rate, FPR, FDR, Fraud Loss Reduction                   ‚ïë\n",
    "‚ïë ‚úì A/B Testing: Compare new rules vs control group                            ‚ïë\n",
    "‚ïë ‚úì Dashboard: Daily, weekly, monthly KPI monitoring                           ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Model performance summary\n",
    "print(\"\\nüìä Final Model Performance Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Best Model: XGBoost\")\n",
    "print(f\"   F1-Score:   {f1_score(y_test, y_pred_xgb)*100:.2f}%\")\n",
    "print(f\"   ROC-AUC:    {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
    "print(f\"   Precision:  {precision_score(y_test, y_pred_xgb)*100:.2f}%\")\n",
    "print(f\"   Recall:     {recall_score(y_test, y_pred_xgb)*100:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Analysis Complete! All 8 questions have been answered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a8600",
   "metadata": {},
   "source": [
    "---\n",
    "# üìé Appendix: Code for Model Deployment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVE MODEL FOR DEPLOYMENT\n",
    "# ============================================\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "model_filename = 'fraud_detection_xgboost_model.pkl'\n",
    "joblib.dump(xgb_model, model_filename)\n",
    "print(f\"‚úÖ Model saved as: {model_filename}\")\n",
    "\n",
    "# Save feature list for inference\n",
    "features_filename = 'model_features.pkl'\n",
    "joblib.dump(selected_features, features_filename)\n",
    "print(f\"‚úÖ Feature list saved as: {features_filename}\")\n",
    "\n",
    "# Example prediction function for deployment\n",
    "def predict_fraud(transaction_data):\n",
    "    \"\"\"\n",
    "    Predict fraud probability for a new transaction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transaction_data : dict\n",
    "        Dictionary containing transaction features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Prediction result with probability\n",
    "    \"\"\"\n",
    "    # Load model and features\n",
    "    model = joblib.load('fraud_detection_xgboost_model.pkl')\n",
    "    features = joblib.load('model_features.pkl')\n",
    "    \n",
    "    # Create DataFrame from input\n",
    "    df_input = pd.DataFrame([transaction_data])\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for col in features:\n",
    "        if col not in df_input.columns:\n",
    "            df_input[col] = 0\n",
    "    \n",
    "    # Predict\n",
    "    df_input = df_input[features]\n",
    "    probability = model.predict_proba(df_input)[0, 1]\n",
    "    prediction = model.predict(df_input)[0]\n",
    "    \n",
    "    return {\n",
    "        'is_fraud': bool(prediction),\n",
    "        'fraud_probability': float(probability),\n",
    "        'risk_level': 'HIGH' if probability > 0.7 else ('MEDIUM' if probability > 0.3 else 'LOW')\n",
    "    }\n",
    "\n",
    "print(\"\\nüìã Example Usage:\")\n",
    "print(\"   result = predict_fraud({'amount': 50000, 'oldbalanceOrg': 100000, ...})\")\n",
    "print(\"   print(result)  # {'is_fraud': True, 'fraud_probability': 0.89, 'risk_level': 'HIGH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced8a6d",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö References & Resources\n",
    "\n",
    "1. **Dataset**: PaySim Synthetic Financial Dataset - [Kaggle](https://www.kaggle.com/datasets/ealaxi/paysim1)\n",
    "2. **XGBoost Documentation**: [XGBoost Docs](https://xgboost.readthedocs.io/)\n",
    "3. **Scikit-learn**: [Documentation](https://scikit-learn.org/stable/)\n",
    "4. **Imbalanced-learn (SMOTE)**: [imblearn Docs](https://imbalanced-learn.org/)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Submission Checklist\n",
    "\n",
    "- [x] **Phase 1**: Environment & Data Loading - Memory optimization implemented\n",
    "- [x] **Phase 2**: Data Cleaning - Missing values, outliers, VIF analysis complete\n",
    "- [x] **Phase 3**: Feature Engineering - Balance errors, one-hot encoding done\n",
    "- [x] **Phase 4**: Model Development - XGBoost and Random Forest trained\n",
    "- [x] **Phase 5**: Performance Evaluation - All metrics calculated and visualized\n",
    "- [x] **Phase 6**: Business Insights - Questions 6, 7, 8 answered\n",
    "- [x] **Visualizations**: Feature importance, ROC-AUC, Confusion Matrix included\n",
    "- [x] **Clean Code**: Well-commented with clear section headings\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created for Internship Task 4 - Fraud Detection Analysis*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
